# -*- coding: utf-8 -*-
"""ForestFireAllModels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8yHzIk1Bosg-_T6kIXfsPNPaLqKJKon
"""

!pip install keras

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

! mkdir ~/.kaggle

!cp /content/drive/MyDrive/Colab_Notebooks/kaggle.json ~/.kaggle/kaggle.json

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d kutaykutlu/forest-fire

!unzip forest-fire.zip

#making a folder
!mkdir training_data
import shutil
original = r'/content/train-smoke'
target = r'/content/training_data'
shutil.move(original, target)
original = r'/content/train_fire'
shutil.move(original, target)

#image augmentation
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

batch_size=32

training_datagenarator= ImageDataGenerator(rescale=1./255,horizontal_flip=True,
    vertical_flip=True,shear_range=0.2,
    zoom_range=0.2,width_shift_range=0.2,
    height_shift_range=0.2,validation_split=0.1)

# import os
# dir = os.listdir('/content/forest_dataset/training_data')
# print(dir)
train=training_datagenarator.flow_from_directory('/content/training_data',
                                                target_size=(224, 224),color_mode='rgb',
                                       class_mode='binary', batch_size=batch_size,subset='training')

validation=training_datagenarator.flow_from_directory('/content/training_data',
                                                target_size=(224, 224),color_mode='rgb',
                                       class_mode='binary', batch_size=batch_size,subset='validation')

# import tensorflow as tf
# import tensorflow_hub as hub
# import numpy as np

# # Define the hyperparameters
# learning_rate = 0.001
# num_epochs = 1
# batch_size = 32

# # Load the EfficientDet model from TensorFlow Hub
# module_url = 'https://tfhub.dev/tensorflow/efficientdet/lite1/detection/1'
# EffiDet = hub.KerasLayer(module_url, trainable=True)

# # Define the model architecture
# EffiDet.build([None, 224, 224, 3])
# EffiDet.summary()

# # Compile the model
# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
# EffiDet.compile(
#     optimizer=optimizer,
#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
#     metrics=['accuracy'],
# )

# # Train the model
# history = EffiDet.fit(
#     train,
#     validation_data=validation,
#     epochs=num_epochs,
# )



# # Save the model
# save_dir = 'path/to/save/directory'
# if not os.path.exists(save_dir):
#     os.makedirs(save_dir)
# model.save(os.path.join(save_dir, 'efficientdet.h5'))

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import os

# Define the hyperparameters
learning_rate = 0.001
num_epochs = 1
batch_size = 32
# Load the EfficientDet model from TensorFlow Hub
module_url = 'https://tfhub.dev/tensorflow/efficientdet/lite1/detection/1'
effnet = hub.KerasLayer(module_url, trainable=True)

# Build the model
model = tf.keras.Sequential([
    effnet,
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(
    optimizer=optimizer,
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'],
)

# Train the model
history = model.fit(
    train,
    validation_data=validation,
    epochs=num_epochs,
)

# Save the model
save_dir = 'path/to/save/directory'
if not os.path.exists(save_dir):
    os.makedirs(save_dir)
model.save(os.path.join(save_dir, 'efficientdet.h5'))

from keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LSTM
from tensorflow.keras.applications import VGG16

# Define a CNN model
cnn=tf.keras.models.Sequential()
cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=[224,224,3]))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2))
cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2))
cnn.add(tf.keras.layers.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2))
cnn.add(tf.keras.layers.Flatten())
cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))
cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))

# # Define a VGG16 model
def create_vgg16_model():
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    for layer in base_model.layers:
        layer.trainable = False
    model = Sequential()
    model.add(base_model)
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='softmax'))
    return model
# Creating FFN model
def create_ffn_model():
    model = Sequential()
    model.add(Flatten(input_shape=(224, 224, 3)))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    return model

#train the ffn model 94.46
ffn_model = create_ffn_model()
ffn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
ffn_model.fit(train, epochs=1, validation_data=validation)

# Train the CNN model
#98.20 accuracy
cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
cnn.fit(train,validation_data=validation,epochs=1,steps_per_epoch=train.samples//batch_size,validation_steps=validation.samples//batch_size
                 )

# Train the VGG16 model
# accuracy 91.75
vgg16_model = create_vgg16_model()
vgg16_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
vgg16_model.fit(train, epochs=1,validation_data=validation)

#accuracy 98.34
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers in the base model
for layer in base_model.layers:
    layer.trainable = False
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dense(64, activation='relu')(x)
predictions = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=predictions)
model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
# Train the model
model.fit(train, epochs=1, validation_data=validation)

from tensorflow.keras.layers import Input, concatenate
# from tensorflow.keras.models import Model

# Define the input layer
input_layer = Input(shape=(224, 224, 3))

# Get the output from each model
cnn_output = cnn(input_layer)
#ffn = ffn_model(input_layer)
v3_output = model(input_layer)
vgg16_output = vgg16_model(input_layer)

# Concatenate the outputs
merged = concatenate([cnn_output, v3_output, vgg16_output])

# Add a fully connected layer
ensemble_output = Dense(10, activation='sigmoid')(merged)

# Define the ensemble model
ensemble_model = Model(inputs=input_layer, outputs=ensemble_output)
ensemble_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the ensemble model
ensemble_model.fit(train, epochs=1,
                   validation_data=validation
                   )

# Evaluate the ensemble model on the test set
# ensemble_model.evaluate(test_)

#MOBILENET GIVING 8% ACCURACY WITH 1 EPOCH
from tensorflow import keras
from keras.applications.mobilenet import MobileNet
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
mobile_net = keras.applications.mobilenet.MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers in the MobileNet model
for layer in mobile_net.layers:
    layer.trainable = False

# Add a classification head to the MobileNet model
x = mobile_net.output
x = GlobalAveragePooling2D()(x)
# x = Dense(64, activation='relu')(x)
predictions = Dense(2, activation='sigmoid')(x)
model = Model(inputs=mobile_net.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train, epochs=1, validation_data=validation)

#RESNET MODEL GIVING 8% ACCURACY WITH 1 EPOCH

from tensorflow import keras
from keras.applications.resnet import ResNet50
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam

resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers in the ResNet model
for layer in resnet.layers:
    layer.trainable = False

# Add a classification head to the ResNet model
x = resnet.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
predictions = Dense(1, activation='softmax')(x)
model = Model(inputs=resnet.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train, epochs=1, validation_data=validation)

# Import required libraries
from keras.applications.inception_v3 import InceptionV3
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator

# Define input shape
input_shape = (224, 224, 3)

# Create InceptionV3 base model
base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)

# Freeze the layers in the base model
for layer in base_model.layers:
    layer.trainable = False

# Add a classification head to the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
# x = Dense(256, activation='sigmoid')(x)
predictions = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(train, epochs=1, validation_data=validation)

# # Evaluate the model
# loss, accuracy = model.evaluate(test_data)
# print('Test loss:', loss)
# print('Test accuracy:', accuracy)

